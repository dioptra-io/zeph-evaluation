{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "*Hypothesis*: Zeph will be able to see almost the same as complete discovery\n",
    "(= full IPv4 routable prefixes) but with a much-reduced probing budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "script_formatter = logging.Formatter(\n",
    "    \"%(asctime)s :: SCRIPT :: %(levelname)s :: %(message)s\"\n",
    ")\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(script_formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# Directory of the experiment\n",
    "exp_dir = Path(\"../resources/data/measurements/exp1-pilot/\")\n",
    "exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directory of the total prefixes and exploitation prefixes pickle files\n",
    "prefixes_dir = exp_dir / \"prefixes\"\n",
    "prefixes_dir.mkdir(parents=True, exist_ok=True)\n",
    "exploitation_dir = exp_dir / \"exploitation\"\n",
    "exploitation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_handler = logging.FileHandler(exp_dir / \"log.txt\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(script_formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "In this section:\n",
    "\n",
    "* we get the configuration of the Iris API and database\n",
    "* we get the configuration of the experiment itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Iris API / database credentials\n",
    "import json\n",
    "from zeph.drivers import create_auth_header, get_database_url\n",
    "\n",
    "config = json.load(open(\"../config.json\"))\n",
    "headers = create_auth_header(config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"])\n",
    "database_url = get_database_url(config[\"iris_url\"], headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "tool = \"yarrp\"\n",
    "epsilon = 0.1\n",
    "protocol = \"icmp\"\n",
    "min_ttl = 8\n",
    "max_ttl = 32\n",
    "\n",
    "measurement_tags = [\"!public\", \"zeph-evaluation\", \"exp1\"]\n",
    "\n",
    "# This can be overrided by the pilot configuration (see below)\n",
    "n_cycles = 10\n",
    "global_budget = 11_881_416\n",
    "\n",
    "# You can generate this file by following these instructions: https://github.com/dioptra-io/zeph#-generate-the-bgp-prefix-file\n",
    "bgp_prefixes_path = Path(\"../resources/data/bgp_prefixes.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot definition (optional)\n",
    "\n",
    "If you don't want to run the experiment on the entire universe of BGP prefixes, you can define a pilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/disable pilot experiment\n",
    "enable_pilot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if not enable_pilot:\n",
    "    with bgp_prefixes_path.open(\"rb\") as fd:\n",
    "        bgp_prefixes = pickle.load(fd)\n",
    "        logger.info(f\"Number of BGP prefixes {len(bgp_prefixes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pilot_bgp_prefixes(bgp_prefixes, n_prefixes):\n",
    "    current_n_prefixes = 0\n",
    "    subset_bgp_prefixes = []\n",
    "\n",
    "    random.shuffle(bgp_prefixes)\n",
    "\n",
    "    for bgp_prefix in bgp_prefixes:\n",
    "        if current_n_prefixes > n_prefixes:\n",
    "            break\n",
    "\n",
    "        subset_bgp_prefixes.append(bgp_prefix)\n",
    "        current_n_prefixes += len(bgp_prefix)\n",
    "\n",
    "    logger.info(f\"Number of /24 prefixes: {current_n_prefixes}\")\n",
    "    return subset_bgp_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally override experiment parameters\n",
    "if enable_pilot:\n",
    "    n_cycles = 3\n",
    "    global_budget = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGP prefixes subset creation\n",
    "\n",
    "Here you can define the subset of BGP prefixes you want to run the pilot experiment on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pilot:\n",
    "    # Enable/diable bgp prefix subset creation\n",
    "    create_bgp_prefixes_subset = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if enable_pilot and not create_bgp_prefixes_subset:\n",
    "    # Override prefix path\n",
    "    bgp_prefixes_path = Path(\"../resources/data/bgp_prefixes_pilot.pickle\")\n",
    "\n",
    "    if not create_bgp_prefixes_subset:\n",
    "        with bgp_prefixes_path.open(\"rb\") as fd:\n",
    "            bgp_prefixes = pickle.load(fd)\n",
    "        logger.info(f\"Number of BGP prefixes {len(bgp_prefixes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pilot and create_bgp_prefixes_subset:\n",
    "        with bgp_prefixes_path.open(\"rb\") as fd:\n",
    "            bgp_prefixes = pickle.load(fd)\n",
    "        bgp_prefixes = pilot_bgp_prefixes(bgp_prefixes, global_budget)\n",
    "        logger.info(f\"Number of BGP prefixes {len(bgp_prefixes)}\")\n",
    "\n",
    "        # Override prefix path\n",
    "        bgp_prefixes_path = Path(\"../resources/data/bgp_prefixes_pilot.pickle\")\n",
    "        with bgp_prefixes_path.open(\"wb\") as fd:\n",
    "            pickle.dump(bgp_prefixes, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance definition\n",
    "\n",
    "In this section we define the instance(s) of the experiment.\n",
    "An instance is one workflow run with a set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeph.main import create_selector\n",
    "from zeph.drivers import iris_driver, get_previous_measurement_agents\n",
    "\n",
    "\n",
    "def adaptive_instance(\n",
    "    name,\n",
    "    n_cycles,\n",
    "    compute_budget,\n",
    "    bgp_prefixes=None,\n",
    "    bgp_awareness=True,\n",
    "    exploitation_only=False,\n",
    "    previous_measurement_uuid=None,\n",
    "    dry_run=False,\n",
    "):\n",
    "    \"\"\"Instance of the experiment.\"\"\"\n",
    "    agents_uuid = None\n",
    "    \n",
    "    for _ in range(n_cycles):\n",
    "\n",
    "        if previous_measurement_uuid:\n",
    "            logger.debug(\"Get previous measurement agents\")\n",
    "            headers = create_auth_header(config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"])\n",
    "            agents_uuid = get_previous_measurement_agents(\n",
    "                config[\"iris_url\"], previous_measurement_uuid, headers\n",
    "            )\n",
    "\n",
    "\n",
    "        selector = create_selector(\n",
    "            database_url, \n",
    "            epsilon, \n",
    "            bgp_prefixes, \n",
    "            previous_measurement_uuid=previous_measurement_uuid,\n",
    "            previous_agents_uuid=agents_uuid,\n",
    "            bgp_awareness=bgp_awareness,\n",
    "        )\n",
    "\n",
    "        measurement_uuid, exploitation_per_agent, prefixes_per_agent = iris_driver(\n",
    "            config[\"iris_url\"],\n",
    "            config[\"iris_username\"],\n",
    "            config[\"iris_password\"],\n",
    "            name,\n",
    "            tool,\n",
    "            protocol,\n",
    "            min_ttl,\n",
    "            max_ttl,\n",
    "            selector,\n",
    "            compute_budget,\n",
    "            logger,\n",
    "            measurement_tags=measurement_tags,\n",
    "            exploitation_only=exploitation_only,\n",
    "            dry_run=dry_run,\n",
    "        )\n",
    "\n",
    "        previous_measurement_uuid = measurement_uuid\n",
    "\n",
    "        recap = {k: len(v) for k, v in prefixes_per_agent.items()}\n",
    "        logger.info(f\"{name} - {measurement_uuid}: {recap}\")\n",
    "\n",
    "        with (exploitation_dir / (\"exploitation_\" + measurement_uuid + \".pickle\")).open(\n",
    "            \"wb\"\n",
    "        ) as fd:\n",
    "            pickle.dump(exploitation_per_agent, fd)\n",
    "        with (prefixes_dir / (\"prefixes_\" + measurement_uuid + \".pickle\")).open(\n",
    "            \"wb\"\n",
    "        ) as fd:\n",
    "            pickle.dump(prefixes_per_agent, fd)\n",
    "        yield measurement_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment definition\n",
    "\n",
    "In this section we define the experiment.\n",
    "Here we have:\n",
    "\n",
    "\n",
    "* zeph cycles (10% of the budget)\n",
    "* zeph cycles (25% of the budget)\n",
    "* zeph cycles (50% of the budget)\n",
    "* zeph cycles (75% of the budget)\n",
    "* zeph cycles (100% of the budget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run, skip the execution\n",
    "dry_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "# Zeph (10%)\n",
    "zeph_uuids_10 = []\n",
    "zeph_10 = adaptive_instance(\n",
    "    \"edgenet-1\",\n",
    "    n_cycles,\n",
    "    lambda _: floor(0.10 * global_budget),\n",
    "    bgp_prefixes=bgp_prefixes,\n",
    "    bgp_awareness=False,\n",
    "    exploitation_only=False,\n",
    "    dry_run=dry_run,\n",
    ")\n",
    "\n",
    "# Zeph (25%)\n",
    "zeph_uuids_25 = []\n",
    "zeph_25 = adaptive_instance(\n",
    "    \"edgenet-2\",\n",
    "    n_cycles,\n",
    "    lambda _: floor(0.25 * global_budget),\n",
    "    bgp_prefixes=bgp_prefixes,\n",
    "    bgp_awareness=False,\n",
    "    exploitation_only=False,\n",
    "    dry_run=dry_run,\n",
    "\n",
    ")\n",
    "\n",
    "# Zeph (50%)\n",
    "zeph_uuids_50 = []\n",
    "zeph_50 = adaptive_instance(\n",
    "    \"edgenet-3\",\n",
    "    n_cycles,\n",
    "    lambda _: floor(0.50 * global_budget),\n",
    "    bgp_prefixes=bgp_prefixes,\n",
    "    bgp_awareness=False,\n",
    "    exploitation_only=False,\n",
    "    dry_run=dry_run,\n",
    ")\n",
    "\n",
    "# Zeph (75%)\n",
    "zeph_uuids_75 = []\n",
    "zeph_75 = adaptive_instance(\n",
    "    \"edgenet-4\",\n",
    "    n_cycles,\n",
    "    lambda _: floor(0.75 * global_budget),\n",
    "    bgp_prefixes=bgp_prefixes,\n",
    "    bgp_awareness=False,\n",
    "    exploitation_only=False,\n",
    "    dry_run=dry_run,\n",
    ")\n",
    "\n",
    "# Agent(s) is probing the full budget\n",
    "zeph_uuids_100 = []\n",
    "zeph_100 = adaptive_instance(\n",
    "    \"edgenet-5\",\n",
    "    n_cycles,\n",
    "    lambda _: global_budget,\n",
    "    bgp_prefixes=bgp_prefixes,\n",
    "    bgp_awareness=False,\n",
    "    exploitation_only=False,\n",
    "    dry_run=dry_run,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment execution\n",
    "\n",
    "We execute the experiment by running the workflow on the instance(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from zeph.drivers import create_auth_header\n",
    "\n",
    "def check_measurement_finished(url, username, password, measurement_uuid):\n",
    "    headers = create_auth_header(url, username, password)\n",
    "    req = requests.get(url + f\"/measurements/{measurement_uuid}\", headers=headers)\n",
    "    return req.json()[\"state\"] == \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "for (\n",
    "    zeph_uuid_10,\n",
    "    zeph_uuid_25,\n",
    "    zeph_uuid_50,\n",
    "    zeph_uuid_75,\n",
    "    zeph_uuid_100,\n",
    "\n",
    ") in zip(\n",
    "    zeph_10,\n",
    "    zeph_25,\n",
    "    zeph_50,\n",
    "    zeph_75,\n",
    "    zeph_100,\n",
    "):\n",
    "\n",
    "    zeph_uuids_10.append(zeph_uuid_10)\n",
    "    zeph_uuids_25.append(zeph_uuid_25)\n",
    "    zeph_uuids_50.append(zeph_uuid_50)\n",
    "    zeph_uuids_75.append(zeph_uuid_75)\n",
    "    zeph_uuids_100.append(zeph_uuid_100)\n",
    "\n",
    "    while True:\n",
    "        check_zeph_10 = check_measurement_finished(\n",
    "            config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"], zeph_uuid_10\n",
    "        )\n",
    "        check_zeph_25 = check_measurement_finished(\n",
    "            config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"], zeph_uuid_25\n",
    "        )\n",
    "        check_zeph_50 = check_measurement_finished(\n",
    "            config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"], zeph_uuid_50\n",
    "        )\n",
    "        check_zeph_75 = check_measurement_finished(\n",
    "            config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"], zeph_uuid_75\n",
    "        )\n",
    "        check_zeph_100 = check_measurement_finished(\n",
    "            config[\"iris_url\"], config[\"iris_username\"], config[\"iris_password\"], zeph_uuid_75\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            check_zeph_10\n",
    "            and check_zeph_25\n",
    "            and check_zeph_50\n",
    "            and check_zeph_75\n",
    "            and check_zeph_100\n",
    "        ):\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "with (exp_dir / \"zeph_10.txt\").open(\"w\") as fd:\n",
    "    for uuid in zeph_uuids_10:\n",
    "        fd.write(uuid + \"\\n\")\n",
    "with (exp_dir / \"zeph_25.txt\").open(\"w\") as fd:\n",
    "    for uuid in zeph_uuids_25:\n",
    "        fd.write(uuid + \"\\n\")\n",
    "with (exp_dir / \"zeph_50.txt\").open(\"w\") as fd:\n",
    "    for uuid in zeph_uuids_50:\n",
    "        fd.write(uuid + \"\\n\")\n",
    "with (exp_dir / \"zeph_75.txt\").open(\"w\") as fd:\n",
    "    for uuid in zeph_uuids_75:\n",
    "        fd.write(uuid + \"\\n\")\n",
    "with (exp_dir / \"zeph_100.txt\").open(\"w\") as fd:\n",
    "    for uuid in zeph_uuids_75:\n",
    "        fd.write(uuid + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a7c0178b9cdc3269eb00ccea0263d7c21cd35e4d7be0ec2e63761aafbb806b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
